\section{Desarrollo}
\label{sec:desarrollo}

\subsection{Algoritmo de Fuerza Bruta}

En ciencias de la computación, la técnica de búsqueda por fuerza bruta o búsqueda exhaustiva, también conocida como generate and test (generar y probar), es una forma muy general de resolver problemas y un paradigma de algoritmos que consiste en enumerar sistemáticamente todos los posibles candidatos a ser solución y luego chequear si cada candidato satisface o no el enunciado del problema. \footnote{$http://faculty.simpson.edu/lydia.sinapova/www/cmsc360/LNotes/L05-BruteForce.htm$}

En nuestro problema "Problema de la Mochila (Gloppi Ya)"  dado un conjunto de pedidos con un peso y beneficio asociado y una mochila de capacidad fija, la solución consiste en el subconjunto de pedidos que maximice la suma de beneficios y a su vez no supere la capacidad de la mochila.

Siguiendo nuestro paradigma de resolución, deberíamos generar todas las posibles soluciones al problema y luego chequear si satisfacen el enunciado. Para ello, generaremos todos los subconjuntos de pedidos (posibles soluciones) y luego nos quedaremos con aquel que maximice la suma de beneficios y a su vez quepa en la mochila. De esta manera siempre encontramos la solución, pues generamos todo el espacio de soluciones y nos quedamos con la mejor.

Para generar todos los subconjuntos posibles de un conjunto de $n$ elementos (espacio de soluciones), podemos observar lo siguiente: Notemos que podemos codificar a cada subconjunto con un numero binario de n bits (cantidad de elementos), en donde el i-esimo bit es $0$ si el elemento en la posición i-esima no pertenece al subconjunto y $1$ en caso contrario.
De esta manera podemos codificar los $2^{n}$ subconjuntos del conjunto de $n$ elementos, ya que con un numero binario de $n$ dígitos se pueden representar $2^{n}$ números.

\begin{algorithm}
\caption{Fuerza Bruta}\label{selection}
\begin{algorithmic}[1]
\Procedure{bruteForce}{$vector \ \ pedidos, \ int \ \ w$}
	\State $vector \ subSets$
   \For{\textbf{$i \gets 1$ to} $2^{|pedidos|}$}
   	\State $wi \gets 0$
	\State $pi \gets 0$
    \For{\textbf{$j \gets 0$ to} $|pedidos|$}
    \If{$j-esimo\ bit\ de\ i\ esta\ en\ 1$}
    	\State $wi \gets wi\ +\ pedidos[j].beneficio$
	\State $pi \gets pi\ +\ pedidos[j].peso$
	\EndIf
	\EndFor

	\If{$wi \ > \ w$}
	\State $pi \gets -1$
	\EndIf

	\State $subSets[i] \gets pi $
    \EndFor


   	\Return{$maxElement(subSets)$}
\EndProcedure
\end{algorithmic}
\end{algorithm}


Al comienzo de nuestra solución propuesta hacemos un ciclo el cual ira desde $1$ hasta $2^{n}$, es decir desde 1 hasta la cantidad de subconjuntos (de esta manera veremos todos los subconjuntos posibles) utilizando a la variable i como un numero de $n$ bits (luego verificaremos que bits están seteados en $1$). 
Para lograr esto, haremos otro ciclo en n, la cantidad de elementos del conjunto (véase también como la cantidad de bits necesarios para representar $2^{n}$ números), en el cual chequearemos que bits de la variable i están en 1 para tener en cuenta esas posiciones del vector pedidos (considerando ese elemento para el subconjunto) para sumar su beneficio y peso (correspondientemente acumulado en las variables wi y pi), generando de esta manera, todos los subconjuntos posibles. 


Finalmente nos preguntamos si la suma de los elementos considerados supera la capacidad de la mochila, de ser así seteamos su beneficio en $-1$ (eliminando la solución), caso contrario almacenamos el beneficio total en el vector subSets. Y por ultimo retornamos el elemento de mayor valor del vector subSets, siendo este el mayor beneficio obtenible.

Con respecto al Análisis de Complejidad temporal, describiremos cada acción con su complejidad y luego realizaremos la suma total.


Complejidad de calcular la suma de todos los elementos de los $2^{n}$ posibles subconjuntos de pedidos que pueden llegar a ser a lo sumo de longitud n = O($n*2^{n}$).


Complejidad de algunos checkeos y comparaciones de cantidades = O(1).


Complejidad Total del algoritmo = O($n*2^{n}$).

\subsection{Backtracking}

Backtracking es una técnica algorítmica que se encarga de encontrar todas las soluciones a un problema, construyendo de a pasos los candidatos a la solución. La ventaja por sobre los algoritmos de fuerza bruta proviene de que podemos eliminar un gran número de candidatos con un único test utilizando podas. Las mismas nos permiten determinar cuándo debemos abandonar a un candidato y que no es posible que alcancen una solución del problema.

Para aplicar backtracking a nuestro problema, lo que notamos es que dado un elemento, una solución puede contenerlo o no. De esta manera, queremos construir nuestro árbol de recurrencias (o árbol de backtracking) en dónde en el paso i, o bien incluímos al i-ésimo elemento en la solución parcial o bien lo excluímos. Esto nos lleva a generar un árbol que contiene a todos los subconjuntos posibles.

El siguiente pseudocódigo muestra como podemos obtener ésto facilmente:

\begin{algorithm}
\caption{Backtracking}\label{selection}
\begin{algorithmic}[1]
\Procedure{backtracking}{$pair \ \ res, \ pair \ \ parcial, \ int \ \ indice$}
   \For{\textbf{$i \gets indice$ to} $|pedidos|$}
   	\State $parcial.w \gets parcial.w + pedidos[i].w$
	  \State $parcial.b \gets parcial.b + pedidos[i].b$
    \If{parcial es mejor que res}
    \State $res \gets parcial$
    \EndIf
    \State Backtracking(res, parcial, i+1)
    \State $parcial.w \gets parcial.w - pedidos[i].w$
	  \State $parcial.b \gets parcial.b - pedidos[i].b$

	 \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Veamos bien como funciona el algoritmo. Las variables res y parcial representan a la solución final y a la solución parcial que estamos estudiando respectivamente, miesntras que índice es el elemento que tenemos que decidir si agregar o no a nuestro candidato. El llamado inicial va a tener a res y a parcial como los pares <0,0>, e índice comenzará en 0. El for recorre todos los elementos que quedan por visitar en un llamado dado (desde índice hasta el final del arreglo de pedidos). En este  ciclo, primero agregamos el elemento a nuestra solución parcial. Si ahora parcial es mejor que res, actualizamos res. Luego, realizamos el llamado recursivo aumentando el índice para recorrer los caminos que incluye al elemento i. Por último, sacamos al elemento i de parcial y continuamos con la próxima iteración, obteniendo así los caminos que no incluyen al elemento i.


Analicemos la complejidad de éste algortimo. Como mencionamos antes, para cada elemento tenemos la decisión de incluirlo o no en nuestra solución parcial. Ésto nos lleva a obtener un árbol binario completo de altura igual a la cantidad de elementos. En otras palabras, tenemos $2^{n}$ caminos de longitud n. Para armar cada camino, fuimos agregando nodos con un costo de O(1), lo que deriva en un costo total de O($2^{n} * n$) para armar todo el árbol de backtracking.

Sin embargo, ésta implementación no aprovecha el uso de podas para poder descartar rápidamente un conjunto de candidatos que no pueden ser la solución final. Para mejorar sobre ésto, vamos a utilizar dos tipos de podas:

\begin{itemize}
\item Poda por factibilidad: éste tipo de podas consiste en descartar una solución parcial que sin importar como continúe su recorrido, no puede cumplir con las condiciones necesarias para ser solución del problema. En nuestro caso, nos fijamos que si un subconjunto tiene un peso mayor al límite W, ya no tiene sentido seguir con el recorrido. Esto nos deja sin explorar todo el subárbol del candidato podado. Ésta técnica podría ser particularmente útil si los elementos nos viniesen ordenados de mayor a menor por su peso. Así, las soluciones que se pasan del límite W lo harían en menos pasos, permitiendonos podar más candidatos.
\item Poda por optimalidad: Cuando no solo importa obtener una solución, si no que buscamos una solución óptima, podemos aplicar éste tipo de podas. Si estamos construyendo un candidato que no tiene posibilidades de ser mejor que la mejor solución encontrada hasta ahora, detenemos el recorrido. En nuestro problema, podemos aplicar una poda de éste tipo sabiendo en cada paso, cuál es el beneficio máximo que podemos agregar a nuestra solución. Si incluso sumando todo ese beneficio no podemos llegar al de nuestra mejor solución, nos detenemos.
\end{itemize}

Notemos que ninguna de estas podas agrega complejidad a nuestro algoritmo original. Para implementar la poda por factibilidad, alcanza con hacer un chequeo de si nuestro peso parcial es mayor al límite W antes de hacer el llamado recursivo. Por otro lado, si mantenemos una variable con la suma de los beneficios de todos los elementos que aún no visitamos, alcanza con que realizemos otro chequeo antes del llamado recursivo, y que vayamos decrementando el beneficio total en cada paso. Cada una de estas cosas tiene una complejidad de O(1).

Como vimos antes, el algoritmo de backtracking sin podas genera todos los subconjuntos posibles, y se queda con el que genera la mejor solución. Veamos que al agregar podas no estamos dejando de lado a la mejor solución del problema. En primer lugar, es fácil ver que la poda por factibilidad solamente desecha candidatos que no son válidos, ya que se exceden del límite de peso W. Por otro otro lado, la poda por optimalidad solo descarta a un candidato una vez que detecta que ya no es posible que logré un beneficio mejor que el de nuestra mejor solución parcial. De esta manera, solamente estamos ignorando ramas del árbol que no llevan a la solución del problema.


\subsection{Algoritmo Meet in the Middle}

La técnica de Meet in the Middle nos permite partir el problema en dos partes de tamaño similar, y luego combinar sus resultados de manera eficiente.
El esquema general de esta técnica es el siguiente:

\begin{enumerate}
\item Partir el problema en dos partes del mismo tamaño.
\item Resolver cada parte por separado, utilizando la técnica de fuerza bruta.
\item Combinar los resultados.
\item Devolver el resultado.
\end{enumerate}

Notemos que si queremos obtener una buena complejidad usando esta técnica, el paso 3) es dónde debemos detenernos con más cuidado. Si no combinamos cuidadosamente los resultados, podríamos obtener una complejidad peor que con fuerza bruta.
Veamos como se aplica éste esquema a nuestro problema:

\begin{enumerate}
\item Separamos todos nuestros elementos en 2 mitades.
\item Para cada mitad, calculamos los pesos y beneficios de todos los subconjuntos posibles.
\item Para cada subconjunto de la primera mitad, buscamos el subconjunto de la segunda mitad tal que se maximize nuestro beneficio.
\item Devolvemos el par con mayor beneficio.
\end{enumerate}

Detengamonos en el paso 3) y veamos la complejidad de éste algoritmo. Sabemos que calcular todos los subconjuntos de un conjunto de cardinal n cuesta O($2^{n}$). En nuestro caso, tenemos dos subconjuntos de cardinal n/2, por lo que la complejidad de los primeros dos pasos es de O($2^{n/2}$). \\
Para implementar el paso 3) hacemos lo siguiente:
\begin{itemize}
\item Ordenamos la segunda mitad según su peso de menor a mayor, y según su beneficio de mayor a menor. Esto tiene una complejidad de O($2^{n/2} * log(2^{n/2})$) = O($2^{n/2} * n/2$).
\item Filtramos la segunda mitad, descartando todos los subconjuntos cuyo peso es mayor al de otros con beneficio mayor o igual. Debido a como ordenamos la segunda mitad, podemos filtrarla en orden lineal, es decir, O($2^{n/2}$).
\item Usamos búsqueda binaria para aparear subconjuntos de cada mitad. La complejidad de esto es la de hacer una búsqueda binaria en un arreglo de tamaño $2^{n/2}$ para $2^{n/2}$ elementos, es decir, O($2^{n/2} * log(2^{n/2})$) = O($2^{n/2} * n/2$).
\end{itemize}

\bigskip

Por lo tanto, vemos que la complejidad total del algoritmo es de O($2^{n/2} * n$). \\

Por último, veamos por qué éste algoritmo es correcto. En primer lugar, es fácil ver que combinando subconjuntos de ambas mitades, podemos generar todos los subconjuntos del conjunto original. Centremosnos en por qué al filtrar una de las mitades, no estamos descartando a la solución del problema. Como ya mencionamos, descartamos los subconjuntos de la segunda mitad cuyo peso es mayor al de otros con beneficio mayor o igual. Supongamos que uno de estos subconjuntos descartados pertenece a la solución final del problema. Llamemoslo S.

Tomemos entonces nuestra solución final, e intercambiemos S por un subconjunto de elementos con menor peso, y beneficio mayor o igual. Esto nos deja con una solución válida, cuyo beneficio es mayor o igual a nuestra solución final. Por lo tanto, descartar S no hubiese alterado nuestra respuesta final.

Por lo tanto, vemos que podemos generar todos los subconjuntos posibles, y que sólo estamos descartando subconjuntos que no alteran el resultado del algoritmo.

\subsection{Algoritmo de Programación Dinámica}
La programación dinámica es un método para reducir el tiempo de ejecución de un algoritmo mediante la utilización de subproblemas superpuestos y subestructuras óptimas\footnote{Una subestructura óptima significa que se pueden usar soluciones óptimas de subproblemas para encontrar la solución óptima del problema en su conjunto.}.

Los subproblemas se resuelven a su vez dividiéndolos en subproblemas más pequeños hasta que se alcance el caso base, donde la solución al problema es trivial.


En resumen, la programación dinámica hace uso de:
\begin{itemize}
	\item Subproblemas superpuestos
	\item Subestructuras óptimas
	\item Memoización (almacenar los resultados obtenidos para no recalcularlos)
\end{itemize}


Ademas, la programación dinámica toma normalmente uno de los dos siguientes enfoques:
\begin{itemize}
	\item Top-down: El problema se divide en subproblemas, y estos se resuelven recordando las soluciones por si fueran necesarias nuevamente. Es una combinación de memoización y recursión.
	\item Bottom-up: Todos los problemas que puedan ser necesarios se resuelven de antemano y después se usan para resolver las soluciones a problemas mayores. Este enfoque es ligeramente mejor en consumo de espacio y llamadas a funciones, pero a veces resulta poco intuitivo encontrar todos los subproblemas necesarios para resolver un problema dado.
\end{itemize}

Para ver que es correcto aplicar programación dinámica en nuestro problema, debemos ver que el problema de la mochila cumple con subproblemas de estructura optima y solapamiento de problemas:
\begin{enumerate}
	\item \textit{Sub-estructura optima:} Al considerar todos los subconjuntos de elementos, para cada elemento tenemos dos casos: $(a)$ el elemento no esta incluido en el conjunto optimo, $(b)$ el elemento esta incluido en el conjunto optimo.
	Por lo tanto, el \textbf{valor máximo} que se puede obtener de $n$ artículos es el \textbf{máximo entre los dos valores siguientes}:
	\begin{enumerate}
		\item \textbf{Valor máximo} obtenido por los $n-1$ artículos restantes y capacidad $W$ (excluyendo el artículo n).
		\item Valor del artículo $n$ más el \textbf{valor máximo} obtenido por los $n-1$ artículos restantes y capacidad $W$ menos el peso del artículo $n$ (incluyendo el artículo n).
	\end{enumerate}
	Si el peso del n-esimo articulo es mayor que $W$, entonces el n-esimo articulo no puede ser incluido y el primer caso es la única posibilidad.
	De esta manera vemos que la solución optima del problema involucra soluciones optimas de los subproblemas.
	
	\item \textit{Solapamiento de problemas:} Siguiendo la estructura recursiva mencionada anteriormente podemos observar que a lo sumo realizamos $O(nxW)$ llamdos recursivos, cuando en realidad tenemos que evaluar $2^{n}$ conjuntos, por lo tanto tenemos solapamiento de subproblemas (en el caso en que $W \in o(2^{n})$).
	%revisar!!!

\end{enumerate}


Siguiendo lo anteriormente dicho podemos plantear la siguiente función matemática recursiva, que resuelve el problema por lo anteriormente detallado:


$f(i,j) =$ el maximo beneficio obtenible dados i pedidos (con su respectivo peso y beneficio) y una mochila con capacidad j


\begin{equation*}
	f(i,j) = \begin{cases}
          0 & i = 0 \lor j = 0 \\
          max(f(i-1,j), beneficio[i-1] + f(i-1, j-peso[i-1])) & peso[i-1] \leq j \\
          f(i-1, j) & en\ otro\ caso
       \end{cases}
\end{equation*}

Nuestra función nos dice que el caso base es cuando tenemos 0 pedidos y una mochila de capacidad 0, por lo tanto el máximo beneficio obtenible es 0.

Sino, tenemos dos caminos a considerar (previo a realizar el llamado recursivo):
\begin{enumerate}
	\item Agregamos el pedido i-ésimo a la mochila si es que este cabe en la misma, sumando el beneficio que este provee
	\item No agregamos el pedido i-ésimo a la mochila por mas que este quepa en la mochila
\end{enumerate}


Como a nosotros nos interesa el beneficio máximo, tomaremos aquel camino que nos maximice el beneficio (motivo por el cual tomamos el máximo).


Por ultimo, en el caso en que el pedido i-ésimo no quepa en la mochila, llamamos directamente al paso recursivo sin incluir al pedido i-ésimo.



Exhibimos el algoritmo propuesto basado en el enfoque bottom-up para mostrar como hacemos uso de la técnica de memoizacion y no tener que recalcular valores anteriormente calculados.

\begin{algorithm}
\caption{Programacion Dinamica}\label{selection}
\begin{algorithmic}[1]
\Procedure{dp}{$vector \ \ pedidos, \ int \ \ w$}
	\State $int \ dp[n+1][w+1]$
   	\For{\textbf{$i \gets 0$ to} $|pedidos|$}
    		\For{\textbf{$j \gets 0$ to} $w$}
    			\If{$i = 0\ \lor \ j=0$}
    				\State $dp[i][j] \gets 0$

    			\Else
				\If{$pedidos[i-1].peso \leq j$}
					\State $dp[i][j] \gets max(dp[i-1][j], pedidos[i-1].beneficio + dp[i-1][j-pedidos[i-1].peso])$
				\Else
					\State $dp[i][j] \gets dp[i-1][j]$
				\EndIf
			\EndIf
		\EndFor
    \EndFor


   	\Return{$dp[n][w]$}
\EndProcedure
\end{algorithmic}
\end{algorithm}


Analicemos la complejidad del algoritmo propuesto. Al iniciar, creamos una matriz de $nxW$ lo que nos da una complejidad de $O(nxW)$.
Luego, podemos ver en el algoritmo que tenemos dos ciclos for anidados, uno que cicla desde 0 hasta n (la cantidad de pedidos) y otro que cicla desde 0 hasta w (el peso de la mochila). Estos ciclos sirven para ir recorriendo nuestra matriz de $nxW$, en donde cada para cada valor de la matriz que no este calculado, este se calcula una única vez y luego se accede al mismo en $O(1)$ para posteriores accesos. Para calcular el mismo, solo se requiere elementos que ya se encuentran calculados debido a la manera en la que recorremos la matriz, por lo tanto se toma el máximo entre ambos, siendo esta operacion $O(1)$. Finalmente, la suma de complejidades nos da $O(nxW)$.
